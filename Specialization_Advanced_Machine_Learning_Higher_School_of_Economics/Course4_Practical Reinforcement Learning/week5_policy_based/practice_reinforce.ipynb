{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in TensorFlow\n",
    "\n",
    "This notebook implements a basic reinforce algorithm a.k.a. policy gradient for CartPole env.\n",
    "\n",
    "It has been deliberately written to be as simple and human-readable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook assumes that you have [openai gym](https://github.com/openai/gym) installed.\n",
    "\n",
    "In case you're running on a server, [use xvfb](https://github.com/openai/gym#rendering-on-a-server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2fe1001cf8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEp5JREFUeJzt3X+s3fV93/Hnq5hAlmQ1hAvy/GMm\njbuGTotBd44jpolC2gKrZio1FbRqUITiLiNSokZboZPWRBtSK61hi9ZZM4HGqbIQRpJiIdqUOURV\n/gjEJI6DcSg3iRPf2sNmAZIsGpvJe3/czyWn5vje43vu9fX95PmQjs73+zmf7/e8P3B43e/93O+H\nk6pCktSfn1ruAiRJS8OAl6ROGfCS1CkDXpI6ZcBLUqcMeEnq1JIFfJJrkzyVZCrJbUv1PpKk4bIU\n98EnOQf4a+AXgWngS8BNVfXkor+ZJGmopbqC3wJMVdU3q+r/AvcC25bovSRJQ6xaovOuBQ4P7E8D\nbzlV54suuqg2bty4RKVI0spz6NAhnn322YxzjqUK+GFF/a25oCTbge0AGzZsYO/evUtUiiStPJOT\nk2OfY6mmaKaB9QP764Ajgx2qamdVTVbV5MTExBKVIUk/uZYq4L8EbEpyaZJXATcCu5fovSRJQyzJ\nFE1VnUjyHuCzwDnAPVV1YCneS5I03FLNwVNVDwEPLdX5JUlzcyWrJHXKgJekThnwktQpA16SOmXA\nS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwk\ndcqAl6ROjfWVfUkOAd8HXgJOVNVkkguBTwIbgUPAr1fVc+OVKUk6XYtxBf8LVbW5qibb/m3Anqra\nBOxp+5KkM2wppmi2Abva9i7ghiV4D0nSPMYN+AL+MsnjSba3tkuq6ihAe754zPeQJC3AWHPwwJVV\ndSTJxcDDSb4+6oHtB8J2gA0bNoxZhiTpZGNdwVfVkfZ8DPgMsAV4JskagPZ87BTH7qyqyaqanJiY\nGKcMSdIQCw74JK9J8rrZbeCXgCeA3cDNrdvNwAPjFilJOn3jTNFcAnwmyex5/ltV/UWSLwH3JbkF\n+A7w9vHLlCSdrgUHfFV9E3jzkPb/BVwzTlGSpPG5klWSOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1\nyoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcM\neEnq1LwBn+SeJMeSPDHQdmGSh5M83Z4vaO1J8uEkU0n2J7liKYuXJJ3aKFfwHwWuPantNmBPVW0C\n9rR9gOuATe2xHdixOGVKkk7XvAFfVX8FfPek5m3Arra9C7hhoP1jNeOLwOokaxarWEnS6BY6B39J\nVR0FaM8Xt/a1wOGBftOt7RWSbE+yN8ne48ePL7AMSdKpLPYfWTOkrYZ1rKqdVTVZVZMTExOLXIYk\naaEB/8zs1Et7Ptbap4H1A/3WAUcWXp4kaaEWGvC7gZvb9s3AAwPt72h302wFXpidypEknVmr5uuQ\n5BPAVcBFSaaB3wf+ALgvyS3Ad4C3t+4PAdcDU8APgXcuQc2SpBHMG/BVddMpXrpmSN8Cbh23KEnS\n+FzJKkmdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLgJalTBrwkdcqAl6RO\nGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpU/MGfJJ7khxL8sRA2weS/E2Sfe1x/cBrtyeZSvJU\nkl9eqsIlSXMb5Qr+o8C1Q9rvrKrN7fEQQJLLgBuBn2/H/Jck5yxWsZKk0c0b8FX1V8B3RzzfNuDe\nqnqxqr4FTAFbxqhPkrRA48zBvyfJ/jaFc0FrWwscHugz3dpeIcn2JHuT7D1+/PgYZUiShllowO8A\nfgbYDBwF/qi1Z0jfGnaCqtpZVZNVNTkxMbHAMiRJp7KggK+qZ6rqpar6EXAXP56GmQbWD3RdBxwZ\nr0RJ0kIsKOCTrBnY/VVg9g6b3cCNSc5LcimwCXhsvBIlSQuxar4OST4BXAVclGQa+H3gqiSbmZl+\nOQT8NkBVHUhyH/AkcAK4tapeWprSJUlzmTfgq+qmIc13z9H/DuCOcYqSJI3PlayS1CkDXpI6ZcBL\nUqcMeEnqlAEvSZ0y4CWpU/PeJin9JHlsx7te0bbl3XctQyXS+LyCl6ROGfCS1CkDXpI6ZcBLUqcM\neEnqlAEvNcPuoJFWMgNekjplwEtSpwx4SeqUAS9JnTLgJalT8wZ8kvVJHklyMMmBJO9t7RcmeTjJ\n0+35gtaeJB9OMpVkf5IrlnoQkqRXGuUK/gTw/qp6E7AVuDXJZcBtwJ6q2gTsafsA1wGb2mM7sGPR\nq5YkzWvegK+qo1X15bb9feAgsBbYBuxq3XYBN7TtbcDHasYXgdVJ1ix65ZKkOZ3WHHySjcDlwKPA\nJVV1FGZ+CAAXt25rgcMDh023tpPPtT3J3iR7jx8/fvqVS5LmNHLAJ3kt8CngfVX1vbm6DmmrVzRU\n7ayqyaqanJiYGLUMSdKIRgr4JOcyE+4fr6pPt+ZnZqde2vOx1j4NrB84fB1wZHHKlSSNapS7aALc\nDRysqg8NvLQbuLlt3ww8MND+jnY3zVbghdmpHEnSmTPKV/ZdCfwW8LUk+1rb7wF/ANyX5BbgO8Db\n22sPAdcDU8APgXcuasWSpJHMG/BV9QWGz6sDXDOkfwG3jlmXJGlMrmSVpE4Z8JLUKQNekjplwEtz\n2PLuu5a7BGnBDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtSpwx4SeqUAS9JnTLg\nJalTBrwkdcqAl6ROGfCS1KlRvnR7fZJHkhxMciDJe1v7B5L8TZJ97XH9wDG3J5lK8lSSX17KAUiS\nhhvlS7dPAO+vqi8neR3weJKH22t3VtV/GOyc5DLgRuDngb8H/I8kP1tVLy1m4ZKkuc17BV9VR6vq\ny237+8BBYO0ch2wD7q2qF6vqW8AUsGUxipWWymM73rXcJUiL7rTm4JNsBC4HHm1N70myP8k9SS5o\nbWuBwwOHTTP3DwRJ0hIYOeCTvBb4FPC+qvoesAP4GWAzcBT4o9muQw6vIefbnmRvkr3Hjx8/7cIl\nSXMbKeCTnMtMuH+8qj4NUFXPVNVLVfUj4C5+PA0zDawfOHwdcOTkc1bVzqqarKrJiYmJccYgSRpi\nlLtoAtwNHKyqDw20rxno9qvAE217N3BjkvOSXApsAh5bvJIlSaMY5S6aK4HfAr6WZF9r+z3gpiSb\nmZl+OQT8NkBVHUhyH/AkM3fg3OodNJJ05s0b8FX1BYbPqz80xzF3AHeMUZckaUyuZJWkThnwktQp\nA16SOmXAS1KnDHhJ6pQBL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEunsOXddy13CdJYDHhJ\n6pQBr24lGfmxFMdLy82Al6ROjfKFH9JPhAePvuvl7V9Z4/y7Vj6v4CX+drgP25dWIgNekjo1ypdu\nn5/ksSRfTXIgyQdb+6VJHk3ydJJPJnlVaz+v7U+11zcu7RAkScOMcgX/InB1Vb0Z2Axcm2Qr8IfA\nnVW1CXgOuKX1vwV4rqreCNzZ+klntZPn3J2DVw9G+dLtAn7Qds9tjwKuBn6jte8CPgDsALa1bYD7\ngf+cJO080lnpLf/yI8BHXt7/d8tXirRoRrqLJsk5wOPAG4E/Br4BPF9VJ1qXaWBt214LHAaoqhNJ\nXgBeDzx7qvM//vjj3kusFc3Pr85GIwV8Vb0EbE6yGvgM8KZh3drzsE/6K67ek2wHtgNs2LCBb3/7\n2yMVLI3qTIauv6BqsU1OTo59jtO6i6aqngc+D2wFVieZ/QGxDjjStqeB9QDt9Z8GvjvkXDurarKq\nJicmJhZWvSTplEa5i2aiXbmT5NXA24CDwCPAr7VuNwMPtO3dbZ/2+uecf5ekM2+UKZo1wK42D/9T\nwH1V9WCSJ4F7k/x74CvA3a3/3cCfJpli5sr9xiWoW5I0j1HuotkPXD6k/ZvAliHt/wd4+6JUJ0la\nMFeySlKnDHhJ6pQBL0md8n8XrG5585Z+0nkFL0mdMuAlqVMGvCR1yoCXpE4Z8JLUKQNekjplwEtS\npwx4SeqUAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6NcqXbp+f5LEkX01yIMkHW/tHk3wryb72\n2Nzak+TDSaaS7E9yxVIPQpL0SqP8/+BfBK6uqh8kORf4QpI/b6/9q6q6/6T+1wGb2uMtwI72LEk6\ng+a9gq8ZP2i757bHXN+ksA34WDvui8DqJGvGL1WSdDpGmoNPck6SfcAx4OGqerS9dEebhrkzyXmt\nbS1weODw6dYmSTqDRgr4qnqpqjYD64AtSf4hcDvwc8A/Bi4Efrd1z7BTnNyQZHuSvUn2Hj9+fEHF\nS5JO7bTuoqmq54HPA9dW1dE2DfMi8CfAltZtGlg/cNg64MiQc+2sqsmqmpyYmFhQ8ZKkUxvlLpqJ\nJKvb9quBtwFfn51XTxLgBuCJdshu4B3tbpqtwAtVdXRJqpckndIod9GsAXYlOYeZHwj3VdWDST6X\nZIKZKZl9wL9o/R8CrgemgB8C71z8siVJ85k34KtqP3D5kParT9G/gFvHL02SNA5XskpSpwx4SeqU\nAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6ZcBLUqcMeEnqlAEvSZ0y4CWpUwa8JHXKgJekThnw\nktQpA16SOmXAS1KnDHhJ6pQBL0mdGjngk5yT5CtJHmz7lyZ5NMnTST6Z5FWt/by2P9Ve37g0pUuS\n5nI6V/DvBQ4O7P8hcGdVbQKeA25p7bcAz1XVG4E7Wz9J0hk2UsAnWQf8M+AjbT/A1cD9rcsu4Ia2\nva3t016/pvWXJJ1Bq0bs9x+Bfw28ru2/Hni+qk60/WlgbdteCxwGqKoTSV5o/Z8dPGGS7cD2tvti\nkicWNIKz30WcNPZO9Dou6Hdsjmtl+ftJtlfVzoWeYN6AT/IrwLGqejzJVbPNQ7rWCK/9uGGm6J3t\nPfZW1eRIFa8wvY6t13FBv2NzXCtPkr20nFyIUa7grwT+eZLrgfOBv8vMFf3qJKvaVfw64EjrPw2s\nB6aTrAJ+GvjuQguUJC3MvHPwVXV7Va2rqo3AjcDnquo3gUeAX2vdbgYeaNu72z7t9c9V1Suu4CVJ\nS2uc++B/F/idJFPMzLHf3drvBl7f2n8HuG2Ecy34V5AVoNex9Tou6HdsjmvlGWts8eJakvrkSlZJ\n6tSyB3ySa5M81Va+jjKdc1ZJck+SY4O3eSa5MMnDbZXvw0kuaO1J8uE21v1Jrli+yueWZH2SR5Ic\nTHIgyXtb+4oeW5LzkzyW5KttXB9s7V2szO51xXmSQ0m+lmRfu7NkxX8WAZKsTnJ/kq+3/9beupjj\nWtaAT3IO8MfAdcBlwE1JLlvOmhbgo8C1J7XdBuxpq3z38OO/Q1wHbGqP7cCOM1TjQpwA3l9VbwK2\nAre2fzcrfWwvAldX1ZuBzcC1SbbSz8rsnlec/0JVbR64JXKlfxYB/hPwF1X1c8Cbmfl3t3jjqqpl\newBvBT47sH87cPty1rTAcWwEnhjYfwpY07bXAE+17f8K3DSs39n+YOYuqV/saWzA3wG+DLyFmYUy\nq1r7y59L4LPAW9v2qtYvy137KcazrgXC1cCDzKxJWfHjajUeAi46qW1FfxaZueX8Wyf/c1/McS33\nFM3Lq16bwRWxK9klVXUUoD1f3NpX5Hjbr++XA4/SwdjaNMY+4BjwMPANRlyZDcyuzD4bza44/1Hb\nH3nFOWf3uGBmseRfJnm8rYKHlf9ZfANwHPiTNq32kSSvYRHHtdwBP9Kq146suPEmeS3wKeB9VfW9\nuboOaTsrx1ZVL1XVZmaueLcAbxrWrT2viHFlYMX5YPOQritqXAOurKormJmmuDXJP52j70oZ2yrg\nCmBHVV0O/G/mvq38tMe13AE/u+p11uCK2JXsmSRrANrzsda+osab5Fxmwv3jVfXp1tzF2ACq6nng\n88z8jWF1W3kNw1dmc5avzJ5dcX4IuJeZaZqXV5y3PitxXABU1ZH2fAz4DDM/mFf6Z3EamK6qR9v+\n/cwE/qKNa7kD/kvApvaX/lcxs1J29zLXtBgGV/OevMr3He2v4VuBF2Z/FTvbJAkzi9YOVtWHBl5a\n0WNLMpFkddt+NfA2Zv6wtaJXZlfHK86TvCbJ62a3gV8CnmCFfxar6n8Ch5P8g9Z0DfAkizmus+AP\nDdcDf83MPOi/We56FlD/J4CjwP9j5ifsLczMZe4Bnm7PF7a+YeauoW8AXwMml7v+Ocb1T5j59W8/\nsK89rl/pYwP+EfCVNq4ngH/b2t8APAZMAf8dOK+1n9/2p9rrb1juMYwwxquAB3sZVxvDV9vjwGxO\nrPTPYqt1M7C3fR7/DLhgMcflSlZJ6tRyT9FIkpaIAS9JnTLgJalTBrwkdcqAl6ROGfCS1CkDXpI6\nZcBLUqf+P+rDhNnPOs1cAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2fe0ad4710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "#gym compatibility: unwrap TimeLimit\n",
    "if hasattr(env,'env'):\n",
    "    env=env.env\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the policy network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
    "\n",
    "For numerical stability, please __do not include the softmax layer into your network architecture__. \n",
    "\n",
    "We'll use softmax or log-softmax where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "#create input variables. We only need <s,a,R> for REINFORCE\n",
    "states = tf.placeholder('float32',(None,)+state_dim,name=\"states\")\n",
    "actions = tf.placeholder('int32',name=\"action_ids\")\n",
    "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#<define network graph using raw tf or any deep learning library>\n",
    "#dc = L.Dense(32,activation='tanh')(states)\n",
    "#dc = L.Dense(8,activation='tanh')(dc)\n",
    "logits = L.Dense(n_actions)(states)\n",
    "\n",
    "#logits = <linear outputs (symbolic) of your network>\n",
    "\n",
    "policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#utility function to pick action in one given state\n",
    "get_action_proba = lambda s: policy.eval({states:[s]})[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get probabilities for parti\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]),actions],axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy,indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# policy objective as in the last formula. please use mean, not sum.\n",
    "# note: you need to use log_policy_for_actions to get log probabilities for actions taken\n",
    "\n",
    "J = tf.reduce_mean(log_policy_for_actions*cumulative_rewards)#<YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#regularize with entropy\n",
    "entropy =  - tf.reduce_sum(policy*log_policy)#<compute entropy. Don't forget the sign!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = []#<a list of all trainable weights in your network>\n",
    "\n",
    "#weight updates. maximizing J is same as minimizing -J. Adding negative entropy.\n",
    "loss = -(J +0.001 * entropy)#0.1\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss,)#var_list = all_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, #rewards at each step\n",
    "                           gamma = 0.99 #discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "    \n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    #<your code here>\n",
    "    cum_rewards = [0] * len(rewards)\n",
    "    cum_rewards[-1] = rewards[-1]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        cum_rewards[i] = rewards[i] + gamma * cum_rewards[i+1]\n",
    "    \n",
    "        \n",
    "    return cum_rewards#<array of cumulative rewards>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "assert len(get_cumulative_rewards(range(100))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,0,0,1,0],gamma=0.9),[1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,-2,3,-4,0],gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0,0,1,2,3,4,0],gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(_states,_actions,_rewards):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
    "    update.run({states:_states,actions:_actions,cumulative_rewards:_cumulative_rewards})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "    \n",
    "    #arrays to record session\n",
    "    states,actions,rewards = [],[],[]\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #action probabilities array aka pi(a|s)\n",
    "        action_probas = get_action_proba(s)\n",
    "        \n",
    "        a = np.random.choice([0,1],p=action_probas)#<pick random action using action_probas>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    train_step(states,actions,rewards)\n",
    "            \n",
    "    return sum(rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 mean reward:22.400\n",
      "1 mean reward:24.300\n",
      "2 mean reward:23.410\n",
      "3 mean reward:23.890\n",
      "4 mean reward:23.320\n",
      "5 mean reward:29.540\n",
      "6 mean reward:26.760\n",
      "7 mean reward:28.620\n",
      "8 mean reward:28.730\n",
      "9 mean reward:32.470\n",
      "10 mean reward:31.470\n",
      "11 mean reward:31.230\n",
      "12 mean reward:32.570\n",
      "13 mean reward:37.190\n",
      "14 mean reward:36.890\n",
      "15 mean reward:36.850\n",
      "16 mean reward:38.950\n",
      "17 mean reward:41.130\n",
      "18 mean reward:35.450\n",
      "19 mean reward:39.740\n",
      "20 mean reward:38.150\n",
      "21 mean reward:44.730\n",
      "22 mean reward:42.120\n",
      "23 mean reward:42.170\n",
      "24 mean reward:47.460\n",
      "25 mean reward:46.550\n",
      "26 mean reward:49.500\n",
      "27 mean reward:48.890\n",
      "28 mean reward:42.890\n",
      "29 mean reward:50.470\n",
      "30 mean reward:45.680\n",
      "31 mean reward:50.960\n",
      "32 mean reward:48.890\n",
      "33 mean reward:52.950\n",
      "34 mean reward:54.290\n",
      "35 mean reward:52.520\n",
      "36 mean reward:55.420\n",
      "37 mean reward:55.820\n",
      "38 mean reward:59.310\n",
      "39 mean reward:58.150\n",
      "40 mean reward:59.500\n",
      "41 mean reward:57.260\n",
      "42 mean reward:61.170\n",
      "43 mean reward:62.800\n",
      "44 mean reward:65.500\n",
      "45 mean reward:59.240\n",
      "46 mean reward:71.200\n",
      "47 mean reward:66.260\n",
      "48 mean reward:73.760\n",
      "49 mean reward:70.640\n",
      "50 mean reward:74.160\n",
      "51 mean reward:85.220\n",
      "52 mean reward:83.260\n",
      "53 mean reward:84.940\n",
      "54 mean reward:92.820\n",
      "55 mean reward:115.500\n",
      "56 mean reward:119.610\n",
      "57 mean reward:132.010\n",
      "58 mean reward:140.130\n",
      "59 mean reward:159.210\n",
      "60 mean reward:167.640\n",
      "61 mean reward:157.970\n",
      "62 mean reward:156.890\n",
      "63 mean reward:163.080\n",
      "64 mean reward:196.720\n",
      "65 mean reward:192.570\n",
      "66 mean reward:204.860\n",
      "67 mean reward:212.250\n",
      "68 mean reward:203.580\n",
      "69 mean reward:205.720\n",
      "70 mean reward:213.180\n",
      "71 mean reward:226.900\n",
      "72 mean reward:180.870\n",
      "73 mean reward:193.150\n",
      "74 mean reward:217.960\n",
      "75 mean reward:202.000\n",
      "76 mean reward:208.290\n",
      "77 mean reward:209.120\n",
      "78 mean reward:225.750\n",
      "79 mean reward:233.270\n",
      "80 mean reward:237.420\n",
      "81 mean reward:229.940\n",
      "82 mean reward:233.680\n",
      "83 mean reward:289.430\n",
      "84 mean reward:246.170\n",
      "85 mean reward:235.350\n",
      "86 mean reward:240.710\n",
      "87 mean reward:263.970\n",
      "88 mean reward:257.320\n",
      "89 mean reward:277.670\n",
      "90 mean reward:259.570\n",
      "91 mean reward:239.710\n",
      "92 mean reward:274.300\n",
      "93 mean reward:261.250\n",
      "94 mean reward:254.480\n",
      "95 mean reward:268.390\n",
      "96 mean reward:280.710\n",
      "97 mean reward:241.350\n",
      "98 mean reward:271.980\n",
      "99 mean reward:263.900\n"
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession(config=config)\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    print (i,\"mean reward:%.3f\"%(np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from submit import submit_cartpole\n",
    "submit_cartpole(generate_session, <EMAIL>, <TOKEN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# That's all, thank you for your attention!\n",
    "# Not having enough? There's an actor-critic waiting for you in the honor section.\n",
    "# But make sure you've seen the videos first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
